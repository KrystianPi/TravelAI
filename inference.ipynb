{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from operator import itemgetter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Define the API Key.\n",
    "API_KEY = config['OPENAI_API']['API_KEY']\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY\n",
    "HF_API_KEY = config['HF_API']['API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gt_id</th>\n",
       "      <th>gt_lat</th>\n",
       "      <th>gt_lon</th>\n",
       "      <th>gt_page_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306683262</td>\n",
       "      <td>41.884000</td>\n",
       "      <td>12.491000</td>\n",
       "      <td>48234033</td>\n",
       "      <td>http://en.wikipedia.org/?curid=48234033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>306683263</td>\n",
       "      <td>41.886000</td>\n",
       "      <td>12.495000</td>\n",
       "      <td>48234033</td>\n",
       "      <td>http://en.wikipedia.org/?curid=48234033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306683264</td>\n",
       "      <td>41.891389</td>\n",
       "      <td>12.480278</td>\n",
       "      <td>48234033</td>\n",
       "      <td>http://en.wikipedia.org/?curid=48234033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306683265</td>\n",
       "      <td>41.893850</td>\n",
       "      <td>12.481940</td>\n",
       "      <td>48234033</td>\n",
       "      <td>http://en.wikipedia.org/?curid=48234033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306683266</td>\n",
       "      <td>41.907222</td>\n",
       "      <td>12.498611</td>\n",
       "      <td>48234033</td>\n",
       "      <td>http://en.wikipedia.org/?curid=48234033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gt_id     gt_lat     gt_lon  gt_page_id  \\\n",
       "0  306683262  41.884000  12.491000    48234033   \n",
       "1  306683263  41.886000  12.495000    48234033   \n",
       "2  306683264  41.891389  12.480278    48234033   \n",
       "3  306683265  41.893850  12.481940    48234033   \n",
       "4  306683266  41.907222  12.498611    48234033   \n",
       "\n",
       "                                       url  \n",
       "0  http://en.wikipedia.org/?curid=48234033  \n",
       "1  http://en.wikipedia.org/?curid=48234033  \n",
       "2  http://en.wikipedia.org/?curid=48234033  \n",
       "3  http://en.wikipedia.org/?curid=48234033  \n",
       "4  http://en.wikipedia.org/?curid=48234033  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Read the MySQL configuration from the JSON file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Extract MySQL connection details\n",
    "mysql_config = config.get('mysql', {})\n",
    "username = mysql_config.get('username', 'default_username')\n",
    "password = mysql_config.get('password', 'default_password')\n",
    "host = mysql_config.get('host', 'localhost')\n",
    "database_name = mysql_config.get('database_name', 'your_database')\n",
    "\n",
    "# Create the MySQL database connection string\n",
    "db_url = f\"mysql+mysqlconnector://{username}:{password}@{host}/{database_name}\"\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Use the engine to connect to the database\n",
    "connection = engine.connect()\n",
    "\n",
    "# Specify the SQL query to retrieve data from a table\n",
    "query = \"SELECT * FROM rome_geo_tags\"\n",
    "\n",
    "# Use Pandas to read data from the database into a DataFrame\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipediaapi.Wikipedia(user_agent=\"krystek.pietrzak@gmail.com\", language='en', extract_format=wikipediaapi.ExtractFormat.WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula \n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "def getURLs(location):\n",
    "    \"\"\"\n",
    "    Returns the 5 closest locations to the given location.\n",
    "    \"\"\"\n",
    "    lat, lon = location\n",
    "    # Calculate distances from the given location to all locations in the dataframe\n",
    "    df['distance'] = df.apply(lambda row: haversine(lat, lon, row['gt_lat'], row['gt_lon']), axis=1)\n",
    "\n",
    "    # Filter locations within 500 meters\n",
    "    close_df = df[df['distance'] <= 0.5]\n",
    "\n",
    "    if close_df.empty:\n",
    "        return [df.nsmallest(1, 'distance')['url'].iloc[0]]\n",
    "    \n",
    "    # Otherwise, return up to 5 closest locations within 500 meters\n",
    "    return close_df.nsmallest(5, 'distance')['url'].tolist()\n",
    "\n",
    "# Example usage:\n",
    "location = (41.8952293,12.4764618)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = getURLs(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract data from the webpage and append it to scraped_data\n",
    "        title = soup.find_all('title')[0].text[:-12]\n",
    "        if title[:4] == 'User' or title[:4] == 'List':\n",
    "           return ''\n",
    "        else:\n",
    "            print(f'Title of the page: {title}')\n",
    "            \n",
    "            page_py = wiki.page(title)\n",
    "            if page_py.exists():\n",
    "                print(f\"Page ID; {page_py.pageid}\")\n",
    "                return page_py.text\n",
    "            else:\n",
    "                print('Page does not exist')\n",
    "                return ''\n",
    "            \n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Largo di Torre Argentina\n",
      "Page ID; 2233857\n",
      "Title of the page: Assassination of Julius Caesar\n",
      "Page ID; 15775663\n",
      "Title of the page: Teatro Argentina\n",
      "Page ID; 4266246\n",
      "Title of the page: San Giuliano dei Fiamminghi\n",
      "Page ID; 24145533\n"
     ]
    }
   ],
   "source": [
    "texts = ''\n",
    "for url in urls:\n",
    "    texts += getText(url) + '\\n'\n",
    "\n",
    "with open('text.txt', 'w') as f:\n",
    "    f.write(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "loader = TextLoader('text.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1210, which is longer than the specified 1000\n",
      "Created a chunk of size 2237, which is longer than the specified 1000\n",
      "Created a chunk of size 1145, which is longer than the specified 1000\n",
      "Created a chunk of size 4066, which is longer than the specified 1000\n",
      "Created a chunk of size 7141, which is longer than the specified 1000\n",
      "Created a chunk of size 5144, which is longer than the specified 1000\n",
      "Created a chunk of size 1447, which is longer than the specified 1000\n",
      "Created a chunk of size 4144, which is longer than the specified 1000\n",
      "Created a chunk of size 1304, which is longer than the specified 1000\n",
      "Created a chunk of size 1251, which is longer than the specified 1000\n",
      "Created a chunk of size 4264, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# Text Splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a travel guide. Answer the question based only on the following context from locations close to you:\n",
    "{context}\n",
    "And on the descripiton of what I currently see:\n",
    "{image_description}\n",
    "Question: {question}\n",
    "Answer this question prioritizing context rather than image description.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\n",
    "    \"context\": itemgetter(\"question\") | retriever, \n",
    "    \"question\": itemgetter(\"question\"), \n",
    "    \"image_description\": itemgetter(\"image_description\")\n",
    "} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/microsoft/git-large-coco\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "output = query(\"Screenshot 2023-10-06 at 15.22.58.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the place you are currently seeing is the Roman Forum in Rome, Italy.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is this place\", \"image_description\": f\"{output}\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
